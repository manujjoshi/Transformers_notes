{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Distillation DistilBERT TinyBERT Data_Augmentation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **BERT Variants II - Based on Knowledge Distillation** \n",
        "### Follow book from Pg. no. 153\n",
        "- One of the\n",
        "challenges with using the pre-trained BERT model is that it is computationally expensive\n",
        "and it is very difficult to run the model with limited resources. The pre-trained BERT model\n",
        "has a large number of parameters and also high inference time, which makes it harder to\n",
        "use it on edge devices such as mobile phones.\n",
        "- To alleviate this issue, we transfer knowledge from a large pre-trained BERT to a small\n",
        "BERT using knowledge distillation.\n",
        "- Next,  we will learn about DistilBERT. With DistilBERT, we will see how to\n",
        "transfer knowledge from a large pre-trained BERT to a small BERT by using knowledge\n",
        "distillation in detail.\n",
        "- In this Notebook, we will learn about the following topics:\n",
        "  - Introducing knowledge distillation \n",
        "  - DistilBERT – the distilled version of BERT \n",
        "  - Introducing TinyBERT \n",
        "  - Transferring knowledge from BERT to neural networks\n",
        "\n",
        "## 1. **Introducing knowledge distillation**\n",
        "  - Knowledge distillation is a model compression technique in which a small model is\n",
        "trained to reproduce the behavior of a large pre-trained model. It is also referred to as\n",
        "teacher-student learning, where the large pre-trained model is the teacher and the small\n",
        "model is the student.\n",
        "\n",
        "## 2. **DistilBERT – the distilled version of BERT**\n",
        "- The pre-trained BERT model has a large number of parameters and also high inference\n",
        "time, which makes it harder to use on edge devices such as mobile phones. To solve this\n",
        "issue, we use DistilBERT, which was introduced by researchers at Hugging Face.\n",
        "DistilBERT is a smaller, faster, cheaper, and lighter version of BERT.\n",
        "- DistilBERT uses knowledge distillation. The ultimate idea of\n",
        "DistilBERT is that we take a large pre-trained BERT model and transfer its knowledge to a\n",
        "small BERT through knowledge distillation. The large pre-trained BERT is called a teacher\n",
        "BERT and the small BERT is called a student BERT. \n",
        "- Since the small BERT (student BERT) acquires its knowledge from the large pre-trained\n",
        "BERT (teacher BERT) through distillation, we can call it small BERT DistilBERT.\n",
        "DistilBERT is 60% faster and its size is 40% smaller compared to large BERT models. Now\n",
        "that we have a basic idea of DistilBERT, let's get into the details and learn how it works.\n",
        "\n",
        "## 3. **Distillation in TinyBERT**\n",
        "- As we learned at the beginning of the section, apart from transferring knowledge from the\n",
        "output layer (prediction layer) of the teacher to the student BERT, here, we transfer\n",
        "knowledge from other layers as well.\n",
        "  - Transformer layer (encoder layer)\n",
        "  - Embedding layer (input layer)\n",
        "  - Prediction layer (output layer)\n",
        "\n",
        "### **Transformer layer distillation**\n",
        "  - Attention-based distillation\n",
        "    - We perform attention-based distillation by minimizing the mean squared error\n",
        "between the attention matrix of the student and teacher. It is also important to know that\n",
        "we use an unnormalized attention matrix, that is, the attention matrix without the softmax\n",
        "function. This is because the unnormalized attention matrix performs better and attains\n",
        "faster convergence in this setting.\n",
        "  - Hidden state-based distillation\n",
        "    - The hidden state is basically\n",
        "the output of the encoder, that is, the representation. So, in hidden-state distillation, we\n",
        "transfer knowledge from the hidden state of the teacher encoder to the hidden state of the\n",
        "student encoder. Let be the hidden state of the student and be the hidden state of the\n",
        "teacher. Then we perform distillation by minimizing the mean squared error between \n",
        " and as expressed here:\n",
        " \n",
        "\n",
        " ### **Embedding layer distillation**\n",
        " - In embedding layer distillation, we transfer knowledge from the embedding layer of the\n",
        "teacher to the embedding layer of the student. Let denote the Es embedding of the student\n",
        "and denote Et the embedding of the teacher, then we train the network to perform\n",
        "embedding layer distillation by minimizing the mean squared error between the\n",
        "embedding of student and teacher.\n",
        "\n",
        "### **Prediction layer distillation**\n",
        "- In prediction layer distillation, we transfer knowledge from the final output layer, which is\n",
        "logits produced by the teacher BERT, to the student BERT. This is similar to the distillation\n",
        "loss we learned about with DistilBERT. \n",
        "- We perform prediction layer distillation by minimizing the cross-entropy loss between the\n",
        "soft target and soft prediction. \n",
        "\n",
        "## **Training the student BERT (TinyBERT)**\n",
        "- In TinyBERT, we will use a two-stage learning framework as follows:\n",
        "  - General distillation\n",
        "  - Task-specific distillation\n",
        "\n",
        "## **The data augmentation method**\n",
        "- Note that to perform distillation at the fine-tuning step, we need more task-specific data\n",
        "points. That is, for task-specific distillation, we need more data points. So we use a data\n",
        "augmentation method to obtain the augmented dataset. We will fine-tune the general\n",
        "TinyBERT with this augmented dataset. The next section will show us how data\n",
        "augmentation is performed.\n",
        "\n",
        "## **Findings**\n",
        "- TinyBERT is 96% effective, 7.5 times smaller, and 9.4 times faster during inference\n",
        "than the BERT-Base model.\n",
        "\n",
        "- DistiBERT gives us almost 97% accurate results as the original BERT-Base model. Since\n",
        "DistilBERT is lighter, we can easily deploy it on any edge device and it is 60% faster at\n",
        "inference compared to the BERT model\n",
        "\n",
        "# **NOTE**:\n",
        "- So far, we have learned how to perform knowledge distillation from a large pre-trained BERT to a smaller BERT. Can we perform knowledge distillation and transfer knowledge from a pre-trained BERT to a simple neural network?\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TegFi8AU6l7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QG6cBwSe6mg7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}